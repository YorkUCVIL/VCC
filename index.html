<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models.">
  <meta name="keywords" content="Interpretability, Multilayer, Concepts">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
          p {
            margin-bottom: 10px;
          }
		  .main-container {
			display: flex;
			justify-content: center;
			max-width: 1000px;
			margin: 0px auto;
		  }

		  .center {
			display: block;
			margin-left: auto;
			margin-right: auto;
			width: 15%;
		  }
          .vertical-divider {
            height: auto; /* Adjust the height as needed */
            width: 1px; /* Thickness of the line */
            background-color: black; /* Color of the line */
            margin: 0 auto; /* Center the line */
          }


		  .video-container-rosetta {
			display: flex;
			justify-content: center; /* Align videos to the center */
			flex-wrap: nowrap; /* Prevents wrapping of items */
		  }

		  .video-wrapper-rosetta {
			flex: 0 0 20%; /* Do not grow, do not shrink, start at 32% width */
			margin: 0 0.5%; /* Provide some space between the videos */
			box-sizing: border-box; /* Include padding and borders in the element's total width and height */
		  }

		  .video-container-single {
			display: flex;
			justify-content: center; /* Align videos to the center */
			flex-wrap: wrap; /* Prevents wrapping of items */
			margin-bottom: 20px; /* Adjust this value to add vertical space between rows */
		  }

		  .video-wrapper-single {
			flex: 0 0 32%; /* Do not grow, do not shrink, start at 32% width */
			margin: 0 0.5%; /* Provide some space between the videos */
			box-sizing: border-box; /* Include padding and borders in the element's total width and height */
		  }

		  .video-title {
			text-align: center; /* Center the title text above the video */
			margin-bottom: 0.3em; /* Space between title and video */
		  }

		  video {
			width: 100%; /* Ensure the video fills its container */
			height: auto;
			display: block; /* Ensures that the video is a block-level element */
		  }

		   /* Style the button that is used to open and close the collapsible content */
			.collapsible {
			background-color: rgb(214, 214, 242);
			color: #444;
			cursor: pointer;
			padding: 18px;
			width: 100%;
			border: none;
			text-align: left;
			outline: none;
			font-size: 15px;
			}

			/* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
			.active, .collapsible:hover {
			background-color: rgb(126, 177, 219);
			}

			/* Style the collapsible content. Note: hidden by default */
			.collapsible-content {
			padding: 0 18px;
			display: none;
			overflow: hidden;
			background-color: #f1f1f1;
			}
    </style>

</head>
<body>

<!--<nav class="navbar" role="navigation" aria-label="main navigation">-->
<!--  <div class="navbar-brand">-->
<!--    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--    </a>-->
<!--  </div>-->
<!--  <div class="navbar-menu">-->
<!--    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
<!--      <a class="navbar-item" href="https://keunhong.com">-->
<!--      <span class="icon">-->
<!--          <i class="fas fa-home"></i>-->
<!--      </span>-->
<!--      </a>-->

<!--      <div class="navbar-item has-dropdown is-hoverable">-->
<!--        <a class="navbar-link">-->
<!--          More Research-->
<!--        </a>-->
<!--        <div class="navbar-dropdown">-->
<!--          <a class="navbar-item" href="https://hypernerf.github.io">-->
<!--            HyperNeRF-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://nerfies.github.io">-->
<!--            Nerfies-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://latentfusion.github.io">-->
<!--            LatentFusion-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://photoshape.github.io">-->
<!--            PhotoShape-->
<!--          </a>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->

<!--  </div>-->
<!--</nav>-->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models</h1>
          <div class="is-size-3 publication-authors">
            CVPR 2024 (Highlight)
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mkowal2.github.io/">Matt Kowal</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://lassonde.yorku.ca/users/wildes">Richard P. Wildes</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://csprofkgd.github.io/">Konstantinos G. Derpanis</a><sup>1,2,3</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>York University,</span>
            <span class="author-block"><sup>2</sup>Samsung AI Centre Toronto</span>
            <span class="author-block"><sup>3</sup>Vector Institute</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.02233"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
<!--               Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=_PW4MQXRsKM"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
<!--               Code Link. -->
              <span class="link-block">
                <a href="https://github.com/YorkUCVIL/VCC"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://mkowal2.github.io/VCC_Demo/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Demo</span>
                  </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="content has-text-justified">
        <h2 class="title is-3">A `connectome' maps every neuron and synapse in a biological neural network. Is it possible
          to visualize all human interpretable concepts and connections in every layer of a DNN? Visual Concept
          Connectomes discover concepts and quantify their interlayer connections in DNNs.</h2>
        </div>
        </div>
      <div style='margin-bottom: 1px; text-align: center;'>
        <img src='img/AllLayerVCC.png'>
      </div>
<!--      <h2 class="subtitle has-text-centered">-->
<!--        Visual Concept Connectomes discover concepts and quantify their interlayer connections in DNNs.-->
<!--      </h2>-->
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Understanding what deep network models capture in their learned representations is a fundamental challenge
            in computer vision. We present a new methodology to understanding such vision models, the Visual Concept
            Connectome (VCC), which discovers human interpretable concepts and their interlayer connections in a
            fully unsupervised manner. Our approach simultaneously reveals fine-grained concepts at a layer,
            connection weightings across all layers and is amendable to global analysis of network structure
            (e.g. branching pattern of hierarchical concept assemblies). Previous work yielded ways to extract
            interpretable concepts from single layers and examine their impact on classification, but did not afford
            multilayer concept analysis across an entire network architecture. Quantitative and qualitative empirical
            results show the effectiveness of VCCs in the domain of image classification. Also, we leverage VCCs for
            the application of failure mode debugging to reveal where mistakes arise in deep networks.

          </p>
        </div>
      </div>
    </div>
<!--    / Abstract. -->




<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Method. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Method Overview</h2>
              <p>
                A VCC is constructed in three main steps as shown in Figure 2. (A) Image segments are extracted in the model’s feature space
                via divisive clustering to produce semantically meaningful image regions for each selected layer.
                (B) Layer-wise concepts, i.e. the nodes of the graph, are discovered in an open world fashion
                (i.e. no labelled data is required) via a second round of clustering over the dataset of image
                regions, independently for each layer.
                (C) Edges are calculated that indicate the contribution of concepts from earlier to deeper layers
                via an approach we introduce, Interlayer Testing with Concept Activation Vectors (ITCAVs) - an
                interlayer generalization of the TCAV method.
              </p>
            <img src='img/Method.png'>

        </div>
      </div>
    </div>

    <!-- Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>
          <h2 class="title is-4">Visualizing concept hierarchies</h2>
            <p>
                Figure 4 shows a three layer VCC for GoogLeNet targeting the class “Jay”. Notice how the inception5b
                ‘bird’ concepts (c51, c52) form as selective weighting of inception4c concepts background (c41),
                bird part (c42, c44) and tree branch (c43), while the inception5b background concepts (c53, c54)
                form differently from weighting of solely inception4c background part concepts (e.g. tree branch (c43)
                and green leaves (c41)). Notably, the network separates subspecies of Jay in the final layer
                (e.g. Blue Jay (c52 ) and other types (c51 )). The concepts found in inception4c are composed from
                varying combinations of colors and parts found in conv3 (e.g. various bird parts (c31, c33) contribute
                to the bird concepts at inception4c). In the end, both scene and object contribute with strong weights
                to the final category.
            </p>
            <div style="text-align: center; margin-bottom: 20px">
                <img src="img/Jay.png" width="75%" height="auto">
            </div>


          <h2 class="title is-4">Subgraph Quantitative Analysis</h2>
            <p>
              We now quantitatively analyze four-layer VCCs generated for the following CNNs and transformers:
              ResNet50, VGG16, MobileNetv3, ViT-b and MViT. Layers are selected approximately uniformly across a
              model to capture concepts at different abstractions. We use standard metrics for analyzing tree-like
              data structures in a per-layer fashion: branching factor (extent to which the representation is
              distributed), number of nodes (how many concepts the model uses to encode a particular class), edge
              weights (strength of connection between concepts) and edge weight variance (variability of connection
              strength).
            </p>

            <p>
              We calculate averages over 50 VCCs for 50 randomly selected ImageNet classes.
              Patterns are apparent in concept numbers and edge weights: more, but weaker,
              concepts in early layers vs. fewer, but stronger, concepts in later layers. These patterns
              reflect the shared low-level features (e.g. colors, textures) across classes and more specific
              features near the end, which yield larger ITCAV values. Also, CNNs show a decreasing branching factor,
              while transformers maintain a consistent number until the final layer, where all models converge to
              about two concepts, typically of an ImageNet class’s foreground-background structure. Transformers
              have higher final layer edge weight variance compared to CNNs, indicating their ability to better
              differentiate earlier concepts’ importance in forming later concepts, potentially explaining their
              superior classification performance (i.e. all information is not equally valuable).
            </p>

            <p>
              We also compare models (ResNet50 and VGG16) when trained with self-supervision (SimCLR) or for adversarial
              robustness, i.e. on Stylized ImageNet (ADV). We observe that robust and self-supervised models
              have fewer low-level concepts and compositionality than the originals, likely as their training yields
              less reliance on texture (stylization perturbs texture) and color (SimCLR training jitters color).
            </p>
            <div style="text-align: center;">
                <img src="img/SubGraphMetrics.png" width="100%" height="auto">
            </div>



          <h2 class="title is-4">CNNs vs. Transformers: Qualitative Analysis</h2>
            <p>
               To examine these patterns further, VCC visualizations for a CNN and transformer subgraphs are shown in Fig. 7.
              Here, we limit to a ResNet50 and MViT and two later layers with the class output. The connection diversity
              in the last layer is indeed observed to be larger for the transformer vs. the CNN. Notably, over half
              of the concepts in the CNN capture background centric concepts, while the transformer has only a single
              background centric concept.
            </p>
            <div style="text-align: center;">
                <img src="img/CNNvsTRANS.png" width="90%" height="auto">
            </div>


          <h2 class="title is-4">All Layer Quantitative Analysis</h2>
            <p>
              Figure 6 presents a quantitative analyses on all layer VCCs for three diverse models: (i) a standard CNN, VGG-16,
              (ii) an efficient model, MobileNetv3 and (iii) a transformer, MViT. Averages are taken over 10 VCCs for
              10 random ImageNet classes. Common trends appear in all models. Concept
              composition is non-linear across layers, with branching factor ranging from 5-15 and converging to
              approximately two near the last layers. The peak number of concepts, around 20, is consistently at 30-40%
              of network depth and, as in the four-layer analysis, also converges to two in the final layer.
              Edge weights and variances are in accord with the main findings of the four-layer analysis, but also
              reveal other insights into the compositional nature of the models. At a fine grained view, all layer analysis,
              each model more readily displays unique edge weight characteristics: VGG16’s average weights decrease
              in later layers, MobileNetv3’s drop greatly before the final layer and MViT maintains consistent values.
              Still, overall these results indicate that penultimate concepts differ between CNNs vs. transformers, as
              supported by our four-layer VCC analysis. Higher variances in initial layers suggest a
              diverse combination of concepts, whereas deeper layers indicate a more uniform composition. Transformers,
              however, show a variance increase in the final layer, indicating greater compositionality.
            </p>
            <div style="text-align: center;">
                <img src="img/ALlLayerMetrics.png" width="70%" height="auto">
            </div>

          <h2 class="title is-4">Model Debugging with VCCs</h2>
            <p>
               To show the VCC’s practical utility, we consider application to model failure analysis. VCCs provide
              insights on compositional concepts across layers and distributed representations.
              Figure 8 shows a ‘Tricycle’ incorrectly classified as a ‘Rickshaw’ by a ResNet50 model,
              and the corresponding incorrect VCC (‘Rickshaw’, left) and correct VCC (‘Tricycle’, right).
              As the image is decomposed using our top-down segmentation, it is revealed that the
              majority of pooled segments are closer, in terms of l2 distance, to concepts in the
              Rickshaw VCC (red outlines) than the tricycle VCC (green outlines). While the model correctly
              encoded the wheel and handlebar portions of the images as tricycle concepts, the background and
              collapsible hood concepts are composed from layers two through four as rickshaw concepts,
              which may cause the error. We also note the lack of other tricycle-specific concepts (e.g. children).

            </p>
            <div style="text-align: center;">
                <img src="img/Debug.png" width="100%" height="auto">
            </div>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{kowal2024visual,
  title={Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models},
  author={Kowal, Matthew and Wildes, Richard P and Derpanis, Konstantinos G},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10895--10905},
  year={2024}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2404.02233.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://yorkucvil.github.io/VCC/" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Project page based on <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies webpage</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
